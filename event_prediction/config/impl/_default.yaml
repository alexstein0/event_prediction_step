# Settings for implementation details
# These settings "should" not influence the outcome of the computation in major ways, only its speed.
# These settings are generic implementation details
# -----------------------------------------------------------------------------------------------------

# This is the main folder where data will be stored (such as caches of datasets and tokenizers):
# This can be an absolute path (which will be honored) or a relative path
# The relative path will be executed relative to the cfg.base_dir
# This behavior is controlled in the main_launcher
path: data

# data implementation:
local_staging_dir: # Optionally copy a preprocessed dataset into this folder before loading it for training
forbid_dataset_preprocessing: True
temporary_corpus: False # Save data directly into local staging dir, forget after use
max_raw_chunk_size: 1e14

# checkpointing and logging:
run_eval_every_nth_step: 10_000
print_loss_every_nth_step: ${impl.run_eval_every_nth_step}
save_intermediate_checkpoints: False
save_every_nth_step: 10_000
which_checkpoints: important # best, latest, important (best+latest) or all

# Basic compute settings
threads: 32 # maximal number of cpu dataloader workers used per GPU, this value will never exceed num_gpus * num_physical threads
# Dataloader multiprocessing
pad_to_multiple_of: 8 # padding in dataloader during downstream

# Default floating point precision:
default_precision: float # needs to be a pytorch datatype

# Distributed training
dist_backend: nccl
sharing_strategy: # file_descriptor # if no argument is given, then the OS default is picked by pytorch

# Misc:
enable_huggingface_offline_mode: False
local_rank: # This is set automatically by the system_startup

save_final_model: False

add_env_variables:
