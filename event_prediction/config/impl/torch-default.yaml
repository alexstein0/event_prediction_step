# Settings for implementation details
# These settings "should" not influence the outcome of the computation in major ways, only its speed.
# These settings are pytorch implementation details, tuned for singl(ish) GPU, sane pytorch stuff
# -----------------------------------------------------------------------------------------------------

name: torch-default
defaults:
  - _default
  - _self_


# Basic pytorch settings
benchmark: True # CUDNN benchmarking
deterministic: False # This option will disable non-deterministic ops
non_blocking: True # unblocked .to(device) handles
tf32_allowed: True
matmul_precision: medium # highest/high/medium

mixed_precision: True # turns on AMP on GPUs/Intel devices. The default precision needs to be float
grad_scaling: True # Only activates when mixed_precision=True
mixed_precision_target_dtype: float16 # you might try your luck with bfloat16 too

# Distributed training:
broadcast_buffers: False
bucket_cap_mb: 25
gradient_as_bucket_view: True
static_graph: True

# scaled dot products:
enable_mem_efficient_sdp: False
enable_math_sdp: True
enable_flash_sdp: True

# Misc:
foreach_optimizer: False

# Compilation
compile_torch: True
mode: default # overwritten by manual selection of inductor variables below
dynamic: False # this is a world of pain (when I last tested it, around torch2.0 release)
fullgraph: True # why even compile when not compile everywhere :>
backend: inductor
_inductor_vars:
